{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8c4004-f8f2-4e38-a301-88b8b6d94331",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We've now talked about why we need machine learning (some relationships are too complex to represent manually), we've looked at the different components of machine learning (model, parameters, loss functions, optimizers), implementing machine learning in a small scale application (linear regression with stochastic gradient descent), and how PyTorch gives us the tools to have more power over our machine learning. Now we are going to combine all of those ideas together to write a model which will identify hand written numbers for us.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "The MNIST dataset is a curated set of hand written numbers that is commonly used as a starting point for machine learning. The original dataset created by NIST (National Institute for Standards and Technology) was leveraged in various goverment tasks related to OCR (Optical Character Recognition). Think tasks like reading tax forms, postal addresses, other manual entry processes. MNIST is a subset of the dataset selected to be more ideal for machine learning with a more sophisticated test/training split. The dataset includes 60,000 images in the training set and 10,000 images in the test set.\n",
    "\n",
    "### CNNs\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a special kind of neural network designed for learning features what can assist in recognizing patterns in typically grid like data. They are inspired by how perception works in the brain, but like all neural networks are a simplifaction.\n",
    "\n",
    "If we remember back to the template matching at the begining, for template matching to work, we need a good starting template. CNNs provide us a way to learn more complex features. Interestingly CNNs still effectivly apply a template matching like algorithm, but leveraging machine learning to learn the templates for us.\n",
    "\n",
    "#### AlexNet\n",
    "\n",
    "CNNs for object detection have been on of the widest applications of machine learning (before LLMs). AlexNet is a CNN developed at the University of Toronto to detect up to 1,000 distinct objects and is often times a starting point for making for specifically tuned CNNs since the heavy lifting of training a lot of the parameters has been taken care of by the team.\n",
    "\n",
    "* For more information on the MNIST dataset: https://en.wikipedia.org/wiki/MNIST_database\n",
    "* MNIST CNN Visualizer: https://adamharley.com/nn_vis/\n",
    "* GoogLeNet Visualizer: https://distill.pub/2017/feature-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e579c-d1ee-4d07-bda3-071cc38fbbea",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa51155b-3e64-4679-b85d-259f2b3209ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# A series of operations that will apply to each image\n",
    "# 1. Convert the image into a PyTorch tensor\n",
    "# 2. Normalize the tensors to have a mean of 0.1307 and stand deviation of 0.3081\n",
    "#    these are already known values for MNIST and is a common practice to normalize\n",
    "#    data as it will stop the features we learn later on from out weighing each other\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Using PyTorch's built in data for MNIST, we are telling it to download the training set\n",
    "train_set = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "\n",
    "# The data loader then specifies how we can load in the downloaded images into Python\n",
    "dataloader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "# Function to display images taken from https://pythonguides.com/pytorch-mnist/\n",
    "def show_images(images, labels):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(10):\n",
    "        axes[i].imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "        axes[i].set_title(f\"Label: {labels[i]}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Now we will pull out 10 images to look at\n",
    "images, labels = next(iter(dataiter))\n",
    "\n",
    "# Show images\n",
    "show_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea591f1b-31e7-40b6-8fac-4755c55aa24b",
   "metadata": {},
   "source": [
    "## Creating the Model Structure\n",
    "\n",
    "PyTorch comes with several layers/operations that will be really helpful fo us to build our NN.\n",
    "\n",
    "Model being leveraged: https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "* Conv2D: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "  Allows the for creating a convolution layer. The layer needs a few pieces of information.\n",
    "  1. How many channels are coming in. This usually refers to things like colors in normal RGB images (which would be 3 channels)\n",
    "  2. How many output channels, which will essentially be the number of different filters we will learn\n",
    "  3. The size of each filter\n",
    "* Dropout: https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "\n",
    "  Lets us randomly \"ignore\" certain neurons. This is helpful to reduce the chance that the neurons learn features that are too similar or overfit.\n",
    "* Linear: https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "  Apply an affine linear transformation\n",
    "* ReLu: https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "\n",
    "  Activation function where the result is evaluated as the max(0, result from the neuron)\n",
    "* Max Pooling: https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "\n",
    "  Provides a way to shrink the feature set into a smaller representation\n",
    "\n",
    "* Log Soft Max:https://docs.pytorch.org/docs/2.8/generated/torch.nn.LogSoftmax.html\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c934fe41-9d62-494c-9a48-b2bf05f74d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # First convolution layer will have a single channel\n",
    "        # the result will be 32 filters that will be learned\n",
    "        # with each filter being a 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "\n",
    "        # Second convolution layer that will take in the 32\n",
    "        # filtered results and produce 64 filters with\n",
    "        # each filter being a 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        # One of the dropout operations we want to learn\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # A second dropout that is at a higher probability\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # First linear layer that will take 9216 features down to 128\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "\n",
    "        # Second linear layer that will take 128 features down to 10\n",
    "        # This is very important because this represents the number \n",
    "        # of outputs (the digits 0 - 9)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First take the image and pass it through our first\n",
    "        # convolution layer\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Next apply the relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Now pass the resulting filtered image through the\n",
    "        # second convolution layer\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Apply another relu activation function\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Down sample the resulting image following\n",
    "        # a 2x2 max pool\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Apply a drop out to reduce chance of \n",
    "        # over fitting\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # We still have out data in a 2D format, but the output\n",
    "        # is 1 dimensional (0 - 9). Flatten will make a single\n",
    "        # dimensional input\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Now that we have a 1D input, we can start to learn how to\n",
    "        # turn our larger vector of features into a detection of\n",
    "        # the digits 0-9\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # Use relu as the activation function again\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply the second drop out to again try to \n",
    "        # reduce the chance of overfitting\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # The final linear layer that will bring down the features\n",
    "        # to 10 in total\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Finally we apply this log softmax\n",
    "        # Soft max creates essentially a probabily \n",
    "        # distribution. We can think of it as \"how likley is the input to be a given digit\"\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0387c42-fefe-44c6-b55d-9e732aa8202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.099682\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: -0.117595\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: -0.310387\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: -0.427612\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: -0.554570\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: -0.527752\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: -0.606336\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: -0.682471\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: -0.674687\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: -0.694653\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: -0.699086\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: -0.742075\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: -0.791396\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: -0.753741\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: -0.749037\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: -0.757938\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: -0.784126\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: -0.731444\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: -0.784849\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: -0.715108\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: -0.743220\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: -0.801079\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: -0.802527\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: -0.828510\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: -0.849169\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: -0.840700\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: -0.796497\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: -0.846596\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: -0.876753\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: -0.816872\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: -0.854090\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: -0.893786\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -0.891306\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: -0.906778\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: -0.857302\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: -0.848373\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: -0.850756\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: -0.898102\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: -0.901452\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: -0.904386\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: -0.890935\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: -0.862115\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: -0.885113\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: -0.908631\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: -0.864443\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: -0.883056\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: -0.851512\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: -0.891236\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: -0.915295\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: -0.918441\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: -0.906157\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: -0.856208\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: -0.849872\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: -0.867074\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: -0.922291\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: -0.903153\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: -0.888889\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: -0.943569\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: -0.954557\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: -0.860989\n",
      "\n",
      "Test set: Average loss:  0.0151, Accuracy 9523/10000  (95%\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: -0.921558\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: -0.948289\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: -0.952038\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: -0.938638\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: -0.925488\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: -0.924888\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: -0.906237\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: -0.941034\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: -0.926498\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: -0.941583\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: -0.930088\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: -0.925788\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: -0.945279\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: -0.950756\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: -0.925374\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: -0.945746\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: -0.920187\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: -0.897030\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: -0.940830\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: -0.946805\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: -0.917012\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: -0.962638\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: -0.946533\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: -0.908673\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: -0.932574\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: -0.961565\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: -0.913226\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: -0.948192\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: -0.939087\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: -0.946170\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: -0.897441\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: -0.932184\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -0.898328\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: -0.926356\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: -0.957866\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: -0.919309\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: -0.962345\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: -0.944179\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: -0.952197\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: -0.944892\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: -0.962766\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: -0.927562\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: -0.919485\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: -0.932177\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: -0.966325\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: -0.909982\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: -0.957521\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: -0.947337\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: -0.931169\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: -0.908437\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: -0.927719\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: -0.931534\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: -0.957770\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: -0.952318\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: -0.977537\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: -0.977209\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: -0.948597\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: -0.921636\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: -0.956509\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: -0.940829\n",
      "\n",
      "Test set: Average loss:  0.0150, Accuracy 9662/10000  (97%\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: -0.981317\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: -0.923064\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: -0.947686\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: -0.970492\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: -0.953345\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: -0.959813\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: -0.952352\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: -0.971151\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: -0.894413\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: -0.947222\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: -0.978239\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: -0.952612\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: -0.974509\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: -0.958259\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: -0.967345\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: -0.943134\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: -0.931640\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: -0.943003\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: -0.944743\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: -0.938469\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: -0.964996\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: -0.956804\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: -0.941179\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: -0.937257\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: -0.962860\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: -0.953415\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: -0.961895\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: -0.980413\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: -0.961177\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: -0.968247\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: -0.953721\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: -0.961845\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -0.946985\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: -0.934008\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: -0.982960\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: -0.960498\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: -0.946987\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: -0.924167\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: -0.936132\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: -0.947888\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: -0.941096\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: -0.972524\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: -0.976774\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: -0.959381\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: -0.961935\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: -0.968995\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: -0.954198\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: -0.974025\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: -0.930668\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: -0.977146\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: -0.965469\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: -0.964951\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: -0.949444\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: -0.945653\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: -0.982956\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: -0.978896\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: -0.937142\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: -0.929524\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: -0.956089\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: -0.962749\n",
      "\n",
      "Test set: Average loss:  0.0148, Accuracy 9765/10000  (98%\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: -0.924150\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: -0.959149\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: -0.984552\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: -0.973951\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: -0.974074\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: -0.923920\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: -0.969694\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: -0.943913\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: -0.963428\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: -0.956211\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: -0.962548\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: -0.968518\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: -0.952072\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: -0.956808\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: -0.941368\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: -0.973876\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: -0.973307\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: -0.976648\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: -0.970351\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: -0.969259\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: -0.974722\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: -0.963678\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: -0.958828\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: -0.953924\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: -0.948046\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: -0.982016\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: -0.970647\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: -0.970959\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: -0.966831\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: -0.954506\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: -0.992318\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: -0.979330\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -0.968161\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: -0.974937\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: -0.974486\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: -0.999571\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: -0.974853\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: -0.957546\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: -0.935211\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: -0.969237\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: -0.956827\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: -0.942027\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: -0.960602\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: -0.941040\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: -0.961247\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: -0.969810\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: -0.964031\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: -0.971738\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: -0.970813\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: -0.960195\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: -0.939134\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: -0.989460\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: -0.968161\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: -0.954207\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: -0.959437\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: -0.960492\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: -0.969892\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: -0.963243\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: -0.987264\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: -0.950020\n",
      "\n",
      "Test set: Average loss:  0.0148, Accuracy 9789/10000  (98%\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: -0.956360\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: -0.947214\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: -0.962604\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: -0.947565\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: -0.979518\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: -0.965931\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: -0.937129\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: -0.974518\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: -0.960974\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: -0.976985\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: -0.964879\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: -0.979259\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: -0.931089\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: -0.973669\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: -0.965971\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: -0.969736\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: -0.969925\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: -0.930992\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: -0.970999\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: -0.941072\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: -0.943196\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: -0.969304\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: -0.984439\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: -0.963917\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: -0.974301\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: -0.945043\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: -0.988631\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: -0.963935\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: -0.986817\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: -0.969661\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: -0.973231\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: -0.982066\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -0.995559\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: -0.974964\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: -0.964196\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: -0.974434\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: -0.961764\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: -0.998657\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: -0.954665\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: -0.952074\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: -0.964517\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: -0.969744\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: -0.976712\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: -0.930347\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: -0.979300\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: -0.970001\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: -0.979751\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: -0.987297\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: -0.955682\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: -0.959318\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: -0.975755\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: -0.981587\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: -0.976223\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: -0.950601\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: -0.963451\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: -0.961126\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: -0.980011\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: -0.951168\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: -0.959134\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: -0.990182\n",
      "\n",
      "Test set: Average loss:  0.0148, Accuracy 9808/10000  (98%\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: -0.980532\n",
      "Train Epoch: 6 [1000/60000 (2%)]\tLoss: -0.982378\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: -0.995951\n",
      "Train Epoch: 6 [3000/60000 (5%)]\tLoss: -0.962842\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: -0.969361\n",
      "Train Epoch: 6 [5000/60000 (8%)]\tLoss: -0.984163\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: -0.949087\n",
      "Train Epoch: 6 [7000/60000 (12%)]\tLoss: -0.973159\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: -0.989830\n",
      "Train Epoch: 6 [9000/60000 (15%)]\tLoss: -0.946045\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: -0.975524\n",
      "Train Epoch: 6 [11000/60000 (18%)]\tLoss: -0.969940\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: -0.963153\n",
      "Train Epoch: 6 [13000/60000 (22%)]\tLoss: -0.980721\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: -0.977716\n",
      "Train Epoch: 6 [15000/60000 (25%)]\tLoss: -0.979903\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: -0.992935\n",
      "Train Epoch: 6 [17000/60000 (28%)]\tLoss: -0.989545\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: -0.973610\n",
      "Train Epoch: 6 [19000/60000 (32%)]\tLoss: -0.973620\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: -0.989816\n",
      "Train Epoch: 6 [21000/60000 (35%)]\tLoss: -0.946228\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: -0.953732\n",
      "Train Epoch: 6 [23000/60000 (38%)]\tLoss: -0.993626\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: -0.999423\n",
      "Train Epoch: 6 [25000/60000 (42%)]\tLoss: -0.966227\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: -0.990317\n",
      "Train Epoch: 6 [27000/60000 (45%)]\tLoss: -0.980351\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: -0.964036\n",
      "Train Epoch: 6 [29000/60000 (48%)]\tLoss: -0.964599\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: -0.970174\n",
      "Train Epoch: 6 [31000/60000 (52%)]\tLoss: -0.964826\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -0.979470\n",
      "Train Epoch: 6 [33000/60000 (55%)]\tLoss: -0.968160\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: -0.968594\n",
      "Train Epoch: 6 [35000/60000 (58%)]\tLoss: -0.970195\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: -0.987009\n",
      "Train Epoch: 6 [37000/60000 (62%)]\tLoss: -0.992709\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: -0.966977\n",
      "Train Epoch: 6 [39000/60000 (65%)]\tLoss: -0.970767\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: -0.990244\n",
      "Train Epoch: 6 [41000/60000 (68%)]\tLoss: -0.951917\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: -0.990286\n",
      "Train Epoch: 6 [43000/60000 (72%)]\tLoss: -0.961244\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: -0.996045\n",
      "Train Epoch: 6 [45000/60000 (75%)]\tLoss: -0.989533\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: -0.966349\n",
      "Train Epoch: 6 [47000/60000 (78%)]\tLoss: -0.974475\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: -0.977440\n",
      "Train Epoch: 6 [49000/60000 (82%)]\tLoss: -0.958685\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: -0.989454\n",
      "Train Epoch: 6 [51000/60000 (85%)]\tLoss: -0.999908\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: -0.993991\n",
      "Train Epoch: 6 [53000/60000 (88%)]\tLoss: -0.967560\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: -0.963721\n",
      "Train Epoch: 6 [55000/60000 (92%)]\tLoss: -0.985160\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: -0.959057\n",
      "Train Epoch: 6 [57000/60000 (95%)]\tLoss: -0.971326\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: -0.950084\n",
      "Train Epoch: 6 [59000/60000 (98%)]\tLoss: -0.968553\n",
      "\n",
      "Test set: Average loss:  0.0148, Accuracy 9859/10000  (99%\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: -0.976354\n",
      "Train Epoch: 7 [1000/60000 (2%)]\tLoss: -0.969048\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: -0.991501\n",
      "Train Epoch: 7 [3000/60000 (5%)]\tLoss: -0.949215\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: -0.977050\n",
      "Train Epoch: 7 [5000/60000 (8%)]\tLoss: -0.982062\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: -0.967403\n",
      "Train Epoch: 7 [7000/60000 (12%)]\tLoss: -0.987365\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: -0.962991\n",
      "Train Epoch: 7 [9000/60000 (15%)]\tLoss: -0.980789\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: -0.942942\n",
      "Train Epoch: 7 [11000/60000 (18%)]\tLoss: -0.958691\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: -0.969404\n",
      "Train Epoch: 7 [13000/60000 (22%)]\tLoss: -0.986825\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: -0.972046\n",
      "Train Epoch: 7 [15000/60000 (25%)]\tLoss: -0.973345\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: -0.987946\n",
      "Train Epoch: 7 [17000/60000 (28%)]\tLoss: -0.961287\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: -0.976315\n",
      "Train Epoch: 7 [19000/60000 (32%)]\tLoss: -0.963924\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: -0.970522\n",
      "Train Epoch: 7 [21000/60000 (35%)]\tLoss: -0.971894\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: -0.992905\n",
      "Train Epoch: 7 [23000/60000 (38%)]\tLoss: -0.953894\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: -0.974315\n",
      "Train Epoch: 7 [25000/60000 (42%)]\tLoss: -0.978764\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: -0.960816\n",
      "Train Epoch: 7 [27000/60000 (45%)]\tLoss: -0.964206\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: -0.999994\n",
      "Train Epoch: 7 [29000/60000 (48%)]\tLoss: -0.975079\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: -0.979518\n",
      "Train Epoch: 7 [31000/60000 (52%)]\tLoss: -0.942537\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -0.966313\n",
      "Train Epoch: 7 [33000/60000 (55%)]\tLoss: -0.963289\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: -0.979969\n",
      "Train Epoch: 7 [35000/60000 (58%)]\tLoss: -0.982902\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: -0.984523\n",
      "Train Epoch: 7 [37000/60000 (62%)]\tLoss: -0.987042\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: -0.969872\n",
      "Train Epoch: 7 [39000/60000 (65%)]\tLoss: -0.975154\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: -0.956379\n",
      "Train Epoch: 7 [41000/60000 (68%)]\tLoss: -0.987865\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: -0.974761\n",
      "Train Epoch: 7 [43000/60000 (72%)]\tLoss: -0.966037\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: -0.976220\n",
      "Train Epoch: 7 [45000/60000 (75%)]\tLoss: -0.968531\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: -0.970510\n",
      "Train Epoch: 7 [47000/60000 (78%)]\tLoss: -0.968477\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: -0.994399\n",
      "Train Epoch: 7 [49000/60000 (82%)]\tLoss: -0.999570\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: -0.979779\n",
      "Train Epoch: 7 [51000/60000 (85%)]\tLoss: -0.960318\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: -0.953576\n",
      "Train Epoch: 7 [53000/60000 (88%)]\tLoss: -0.988714\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: -0.994197\n",
      "Train Epoch: 7 [55000/60000 (92%)]\tLoss: -0.970599\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: -0.962461\n",
      "Train Epoch: 7 [57000/60000 (95%)]\tLoss: -0.979983\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: -0.953540\n",
      "Train Epoch: 7 [59000/60000 (98%)]\tLoss: -0.971687\n",
      "\n",
      "Test set: Average loss:  0.0148, Accuracy 9843/10000  (98%\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: -0.970072\n",
      "Train Epoch: 8 [1000/60000 (2%)]\tLoss: -0.979828\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: -0.991342\n",
      "Train Epoch: 8 [3000/60000 (5%)]\tLoss: -0.959392\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: -0.949539\n",
      "Train Epoch: 8 [5000/60000 (8%)]\tLoss: -0.978734\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: -0.956765\n",
      "Train Epoch: 8 [7000/60000 (12%)]\tLoss: -0.991356\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: -0.995461\n",
      "Train Epoch: 8 [9000/60000 (15%)]\tLoss: -0.986715\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: -0.968803\n",
      "Train Epoch: 8 [11000/60000 (18%)]\tLoss: -0.955491\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: -0.969826\n",
      "Train Epoch: 8 [13000/60000 (22%)]\tLoss: -0.958681\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: -0.976619\n",
      "Train Epoch: 8 [15000/60000 (25%)]\tLoss: -0.983849\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: -0.989992\n",
      "Train Epoch: 8 [17000/60000 (28%)]\tLoss: -0.961343\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: -0.977219\n",
      "Train Epoch: 8 [19000/60000 (32%)]\tLoss: -0.949409\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: -0.956110\n",
      "Train Epoch: 8 [21000/60000 (35%)]\tLoss: -0.988419\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: -0.971120\n",
      "Train Epoch: 8 [23000/60000 (38%)]\tLoss: -0.972216\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: -0.998203\n",
      "Train Epoch: 8 [25000/60000 (42%)]\tLoss: -0.947887\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: -0.962798\n",
      "Train Epoch: 8 [27000/60000 (45%)]\tLoss: -0.989580\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: -0.968815\n",
      "Train Epoch: 8 [29000/60000 (48%)]\tLoss: -0.973329\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: -0.976174\n",
      "Train Epoch: 8 [31000/60000 (52%)]\tLoss: -0.980712\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -0.988510\n",
      "Train Epoch: 8 [33000/60000 (55%)]\tLoss: -0.970320\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: -0.970157\n",
      "Train Epoch: 8 [35000/60000 (58%)]\tLoss: -0.978884\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: -0.981618\n",
      "Train Epoch: 8 [37000/60000 (62%)]\tLoss: -0.971706\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: -0.981256\n",
      "Train Epoch: 8 [39000/60000 (65%)]\tLoss: -0.980764\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: -0.979186\n",
      "Train Epoch: 8 [41000/60000 (68%)]\tLoss: -0.993030\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: -0.980077\n",
      "Train Epoch: 8 [43000/60000 (72%)]\tLoss: -0.967552\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: -0.942434\n",
      "Train Epoch: 8 [45000/60000 (75%)]\tLoss: -0.976306\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: -0.989969\n",
      "Train Epoch: 8 [47000/60000 (78%)]\tLoss: -0.979284\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: -0.978976\n",
      "Train Epoch: 8 [49000/60000 (82%)]\tLoss: -0.979840\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: -0.962632\n",
      "Train Epoch: 8 [51000/60000 (85%)]\tLoss: -0.954792\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: -0.969180\n",
      "Train Epoch: 8 [53000/60000 (88%)]\tLoss: -0.997659\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: -0.958881\n",
      "Train Epoch: 8 [55000/60000 (92%)]\tLoss: -0.957833\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: -0.992301\n",
      "Train Epoch: 8 [57000/60000 (95%)]\tLoss: -0.999585\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: -0.977933\n",
      "Train Epoch: 8 [59000/60000 (98%)]\tLoss: -0.974533\n",
      "\n",
      "Test set: Average loss:  0.0147, Accuracy 9861/10000  (99%\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: -0.915320\n",
      "Train Epoch: 9 [1000/60000 (2%)]\tLoss: -0.965697\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: -0.963759\n",
      "Train Epoch: 9 [3000/60000 (5%)]\tLoss: -0.970224\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: -0.991930\n",
      "Train Epoch: 9 [5000/60000 (8%)]\tLoss: -0.991466\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: -0.951562\n",
      "Train Epoch: 9 [7000/60000 (12%)]\tLoss: -0.970464\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: -0.969966\n",
      "Train Epoch: 9 [9000/60000 (15%)]\tLoss: -0.990482\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: -0.999984\n",
      "Train Epoch: 9 [11000/60000 (18%)]\tLoss: -0.989634\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: -0.982178\n",
      "Train Epoch: 9 [13000/60000 (22%)]\tLoss: -0.953744\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: -0.989851\n",
      "Train Epoch: 9 [15000/60000 (25%)]\tLoss: -0.989593\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: -0.988008\n",
      "Train Epoch: 9 [17000/60000 (28%)]\tLoss: -0.948606\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: -0.966346\n",
      "Train Epoch: 9 [19000/60000 (32%)]\tLoss: -0.958420\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: -0.982633\n",
      "Train Epoch: 9 [21000/60000 (35%)]\tLoss: -0.980510\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: -0.968527\n",
      "Train Epoch: 9 [23000/60000 (38%)]\tLoss: -0.962151\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: -0.989921\n",
      "Train Epoch: 9 [25000/60000 (42%)]\tLoss: -0.993014\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: -0.975242\n",
      "Train Epoch: 9 [27000/60000 (45%)]\tLoss: -0.979894\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: -0.990060\n",
      "Train Epoch: 9 [29000/60000 (48%)]\tLoss: -0.948633\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: -0.970237\n",
      "Train Epoch: 9 [31000/60000 (52%)]\tLoss: -0.961370\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -0.961251\n",
      "Train Epoch: 9 [33000/60000 (55%)]\tLoss: -0.977382\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: -0.958726\n",
      "Train Epoch: 9 [35000/60000 (58%)]\tLoss: -0.989962\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: -0.942296\n",
      "Train Epoch: 9 [37000/60000 (62%)]\tLoss: -0.964787\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: -0.989511\n",
      "Train Epoch: 9 [39000/60000 (65%)]\tLoss: -0.953544\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: -0.951394\n",
      "Train Epoch: 9 [41000/60000 (68%)]\tLoss: -0.975569\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: -0.980045\n",
      "Train Epoch: 9 [43000/60000 (72%)]\tLoss: -0.968489\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: -0.976631\n",
      "Train Epoch: 9 [45000/60000 (75%)]\tLoss: -0.981575\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: -0.969880\n",
      "Train Epoch: 9 [47000/60000 (78%)]\tLoss: -0.990117\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: -0.970025\n",
      "Train Epoch: 9 [49000/60000 (82%)]\tLoss: -0.966083\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: -0.968021\n",
      "Train Epoch: 9 [51000/60000 (85%)]\tLoss: -0.985191\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: -0.968665\n",
      "Train Epoch: 9 [53000/60000 (88%)]\tLoss: -0.998700\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: -0.988919\n",
      "Train Epoch: 9 [55000/60000 (92%)]\tLoss: -0.965655\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: -0.970417\n",
      "Train Epoch: 9 [57000/60000 (95%)]\tLoss: -0.971171\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: -0.980431\n",
      "Train Epoch: 9 [59000/60000 (98%)]\tLoss: -0.989802\n",
      "\n",
      "Test set: Average loss:  0.0148, Accuracy 9863/10000  (99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='../data',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CustomCNN().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss: 0.4f}, Accuracy {correct}/{len(loaders['test'].dataset)}  ({100 * correct / len(loaders['test'].dataset):.0f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861b1a4-d55f-4567-981d-c83b6d8b4949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
