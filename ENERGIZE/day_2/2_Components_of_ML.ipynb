{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60fbff2-a112-456e-8ec4-ade49c478847",
   "metadata": {},
   "source": [
    "# Components of Machine Learning\n",
    "\n",
    "## Definitions\n",
    "\n",
    "Machine Learning can be broken down into a few concepts, we will look at examples of each and discuss some of the nuances as we go.\n",
    "\n",
    "* Model: Some representation of a solution. Can be an equation, image template, some format that encapsulates our method of solving the problem at hand.\n",
    "* Parameters: Control values that are specific inputs into our model. They can be things like coefficents to the equation that is the model, weights, or other descriptors.\n",
    "* Loss Function: Some method of measuring how far off an output is front some measure of truth. This can be things like distance (imagine measuring how far a dart is from a bullseye)\n",
    "* Optimizer: Method for determining how to tweak our parameters to reduce our loss function.\n",
    "\n",
    "You can think of the goal of machine learning is to start with some **model** we believe can solve a problem. We then determine the specific **parameters** that are part of the model which produces the smallest **loss** by using our **optimizer**.\n",
    "\n",
    "## Basic Example\n",
    "\n",
    "Lets put this into context using a basic statisical tool. Linear Regression.\n",
    "\n",
    "We will begin by generating some vaguely linear data with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5cb83-2475-4bbb-ab33-5d8e8354662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear algebra library\n",
    "import numpy as np\n",
    "\n",
    "# Visualization library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Our x data is going to be number 0 - 2\n",
    "x = 2 * np.random.rand(100, 1)\n",
    "\n",
    "# We will then make a basic linear function following y = mx + b\n",
    "y = 2 * x.ravel() + 3\n",
    "\n",
    "# But to make things more interesting, lets add some Guassian noise (mean of 0 and standard deviation of 0.05)\n",
    "noise = np.random.normal(0, 0.05, len(x))\n",
    "\n",
    "# And produce our final, noisy y-values\n",
    "y_noisy = y + noise\n",
    "\n",
    "# Let's see our data\n",
    "plt.plot(x, y_noisy, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238dcdba-cf49-4ecb-b11d-bc5185942c53",
   "metadata": {},
   "source": [
    "So our goal will be to learn a \"model\" that represents our data. Now since we generated the data we know it is linear. But if we didn't we would start with our best judgement.\n",
    "Applying the vocab words from above we get this.\n",
    "\n",
    "* Model: Our model will be the slop-intercept form of a line `y = mx + b`\n",
    "* Parameters: For our model, the parameters are `m` and `b`. That is what we are trying to find\n",
    "* Loss Function: The loss will be squared error which will be how far our model + parameters are off from the actual data\n",
    "* Optimizer: For this case we will use stochastic gradient descent. We will talk a bit more about what that means in a second\n",
    "\n",
    "Optimizers\n",
    "* Gradient descent is a method of finding the minimum (or possible maximum) value by iteratively updating parameters based on the gradient of the loss function.\n",
    "* Stochastic gradient descent is gradient descent where a sub-portion of our test points are used for calculating the loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44430a-efb6-4f8b-a86e-41910afa5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the linear regression model\n",
    "model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Train the model on the train dataset\n",
    "model.fit(x, y_noisy)\n",
    "\n",
    "# Lets see what parameters the regressor came up with\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_[0]\n",
    "print(f'Equation: y = {slope:.2f}x + {intercept:.2f}')\n",
    "\n",
    "# Lets plop the expected vs actual\n",
    "plt.scatter(x, y_noisy)\n",
    "\n",
    "test_predictions = model.predict(x)\n",
    "plt.plot(x.ravel(), test_predictions.astype(float), color=\"red\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b09d2-74e2-4e90-83d8-e6a9fa657f32",
   "metadata": {},
   "source": [
    "## Overfitting and Test/Train Splits\n",
    "\n",
    "This will produce a working solution, but this approach is susceptible to a problem in data science and machine learning called \"overfitting\". Remember the objective is to find a model that generally applies to examples we will find out in the wild and not just on the data we have available to us.\n",
    "\n",
    "Let's apply this basic example to the real world to see the problem of overfitting.\n",
    "\n",
    "> You are producing a model that should predict the price of a house in Beacon Hill based on the square footage. You have data on previous house prices from Zillow.\n",
    "\n",
    "We could apply our linear regression model in order to try to get a fit, but if we push our model to perfectly fit our test data, we may actually have a model that doesn't perform as well on real world data.\n",
    "\n",
    "In order to evaluate our model more holistically, we typically take our initial data and perform a \"test, train split\". All that means is we take our data, split it into two categorites. \"Training data\" is what we will provide to the fit operation, allowing the optimizer to update our parameters. When then evaluate our model on our \"test data\" which doesn't impact our parameters and instead is just used as a benchmark to evaluate performace.\n",
    "\n",
    "Below is the code tweaked to make use of a test/train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567e2f2-26be-43be-abf0-4c4307e2ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Create the linear regression model\n",
    "model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Split our data into a test and train set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_noisy, test_size=0.2)\n",
    "\n",
    "# Train the model on the train dataset\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Lets see what parameters the regressor came up with\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_[0]\n",
    "print(f'Equation: y = {slope:.2f}x + {intercept:.2f}')\n",
    "\n",
    "# Lets plop the expected vs actual\n",
    "plt.scatter(x, y_noisy)\n",
    "\n",
    "test_predictions = model.predict(x)\n",
    "plt.plot(x.ravel(), test_predictions.astype(float), color=\"red\")\n",
    "\n",
    "# Now we can measure the performance using our test data\n",
    "test_predictions = model.predict(x_test)\n",
    "mean_error = mean_absolute_error(y_test, test_predictions)\n",
    "percent_error = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "print('Error: ', mean_error)\n",
    "print('Percent Error: ', percent_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f751e-f7e4-49bf-b423-a4a0fe5fe80b",
   "metadata": {},
   "source": [
    "Before we move on, its worth noting that our \"x\" data doesn't have to be a single dimension. Linear models can be fit to high-dimensional data. Sklearn's SGDRegressor can handle hundreds of dimensions as input. So if we consider the example of house cost estimation, we could fit a linear model based on square footage, distance to the common, number of bedrooms, number of bathrooms, age of the house, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
